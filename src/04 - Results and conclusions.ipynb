{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = joblib.load('../output/modelizationResults.pkl')\n",
    "final_results['Fine_Tuning'] = final_results['Fine_Tuning'].astype('bool')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the populated results of the different algorithms applied to the dataset. As you can see, for each algorithm we provide scores ($R^2$ and MAE) for a prediction with default settings and scores with hyperparameters tuned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Fine_Tuning</th>\n",
       "      <th>R2_Train</th>\n",
       "      <th>R2_Test</th>\n",
       "      <th>MAE_Train</th>\n",
       "      <th>MAE_Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision trees</td>\n",
       "      <td>False</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.863</td>\n",
       "      <td>269.0</td>\n",
       "      <td>1278.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision trees</td>\n",
       "      <td>True</td>\n",
       "      <td>0.943</td>\n",
       "      <td>0.883</td>\n",
       "      <td>926.0</td>\n",
       "      <td>1263.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>False</td>\n",
       "      <td>0.975</td>\n",
       "      <td>0.907</td>\n",
       "      <td>572.0</td>\n",
       "      <td>1132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>True</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.881</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>1445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-nearest neighbours</td>\n",
       "      <td>False</td>\n",
       "      <td>0.900</td>\n",
       "      <td>0.839</td>\n",
       "      <td>1131.0</td>\n",
       "      <td>1445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-nearest neighbours</td>\n",
       "      <td>True</td>\n",
       "      <td>0.989</td>\n",
       "      <td>0.860</td>\n",
       "      <td>273.0</td>\n",
       "      <td>1276.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>False</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.836</td>\n",
       "      <td>1519.0</td>\n",
       "      <td>1556.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>True</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.911</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>1172.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Algorithm  Fine_Tuning  R2_Train  R2_Test  MAE_Train  MAE_Test\n",
       "0        Decision trees        False     0.989    0.863      269.0    1278.0\n",
       "0        Decision trees         True     0.943    0.883      926.0    1263.0\n",
       "0         Random Forest        False     0.975    0.907      572.0    1132.0\n",
       "0         Random Forest         True     0.897    0.881     1382.0    1445.0\n",
       "0  K-nearest neighbours        False     0.900    0.839     1131.0    1445.0\n",
       "0  K-nearest neighbours         True     0.989    0.860      273.0    1276.0\n",
       "0     Gradient Boosting        False     0.847    0.836     1519.0    1556.0\n",
       "0     Gradient Boosting         True     0.931    0.911     1108.0    1172.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can observe, results are not exactly the same amongst different algorithms, so we have to examinate carefully which one could provide better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decission trees, despite of getting good scores with train data is not so good with test data. There is a substantial gap between scores. Same happens with KNN. This is probably a signal of overfitting, so we will discard them as a solution for our issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two algorithms that provide the smallest gap between scores in train and test (tuned random forest and gradient boosting). We would accept a little higher error in estimation of train data (higher than decision trees or KNN, for instance) but having the best score in test data. This would be the best way to avoid the suspicion of overfitting. These would be the best options to choose from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>Fine_Tuning</th>\n",
       "      <th>R2_Train</th>\n",
       "      <th>R2_Test</th>\n",
       "      <th>MAE_Train</th>\n",
       "      <th>MAE_Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>True</td>\n",
       "      <td>0.897</td>\n",
       "      <td>0.881</td>\n",
       "      <td>1382.0</td>\n",
       "      <td>1445.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>True</td>\n",
       "      <td>0.931</td>\n",
       "      <td>0.911</td>\n",
       "      <td>1108.0</td>\n",
       "      <td>1172.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Algorithm  Fine_Tuning  R2_Train  R2_Test  MAE_Train  MAE_Test\n",
       "0      Random Forest         True     0.897    0.881     1382.0    1445.0\n",
       "0  Gradient Boosting         True     0.931    0.911     1108.0    1172.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results.iloc[[3,7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to tell which one would work better, but another aspect to keep in mind when choosing the best algorithm should be the analysis of the feature importances in the model. If results fit with our knowledge about the structure of the data we will be more confident with the choice. Let's see results for each algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../output/featImpRF.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "     \n",
    "<img src=\"../output/featImpGB.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With random forest, only powerPS and year concentrate almost all importance in the data explanation. However, gradient boosting gives importance to other features like kilometers and brand & model (actually, the most important feature for the algorithm). It's known that brand and model are important in cars market and they have an remarkable effect on the price. Just as a reminder, we show again the boxplots by brand chart drawn at data visualization section:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../output/brandsBoxplot.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in conclusion, gradient boosting will be, finally, our choice for deploying the predictive model as it is the model that best predicts test prices without discarding the importance of some features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some caveats**:  \n",
    "Although getting very acceptable results, there is some aspect that we have to consider in further improvements of the model.  \n",
    "We saw that prices in dataset covered a wide range (for the analysis we cut from 200€ to 100K€). It is such a huge spread, so we knew in advance that it would be hard to obtain optimal results.  \n",
    "Amongst all used algorithms, we observed the same phenomenon: As the cars were more expensive, more error we got in the prediction. That is, model predicted poorly cars above 40K€ and tended to underestimate them.  \n",
    "Some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../output/DTTest.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "<img src=\"../output/RFTest.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "<img src=\"../output/KNNTest.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "<img src=\"../output/GBTest.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's true that these cases are only a few (only 0.5% of samples above 40K€), but it would be nice to improve the performance of the prediction in the future. Actually, I think I will postpone it after the summer :-)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
